{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons as tfa\n",
    "# !pip install focal-loss\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from focal_loss import BinaryFocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1399 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n",
      "Found 301 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# glowny folder\n",
    "base_dir = '/Users/milenabiernacka/Desktop/studia/DS/semestr2/PD-magisterka/Mushroom_dataset/cnn/'\n",
    "# polaczenie glownego folderu i podfolderow z edible i poisonous\n",
    "\n",
    "edible_dir = os.path.join(base_dir, 'Edible')\n",
    "poisonous_dir = os.path.join(base_dir, 'Poisonous')\n",
    "\n",
    "# Assuming base_dir is the path to the dataset directory\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_generator(generator):\n",
    "    for batch_x, batch_y in generator:\n",
    "        yield [batch_x, batch_y], batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_wrapper = wrapper_generator(train_generator)\n",
    "validation_gen_wrapper = wrapper_generator(validation_generator)\n",
    "test_gen_wrapper = wrapper_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object wrapper_generator at 0x35d37b3e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_classes=2, s=30.0, m=0.50, **kwargs):\n",
    "        super(ArcFaceLossLayer, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s  # Scale parameter\n",
    "        self.m = m  # Margin parameter\n",
    "        self.cos_m = tf.cos(m)\n",
    "        self.sin_m = tf.sin(m)\n",
    "        self.th = tf.cos(tf.constant(np.pi) - m)\n",
    "        self.mm = tf.sin(tf.constant(np.pi) - m) * m\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[-1], self.n_classes),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 dtype='float32')\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        # Normalize feature vectors and weights\n",
    "        x = tf.nn.l2_normalize(inputs, axis=1)\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # Dot product\n",
    "        logits = x @ W\n",
    "        # Add margin\n",
    "        theta = tf.acos(tf.clip_by_value(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        logits = logits * (1 - labels) + target_logits * labels\n",
    "        # Rescale logits\n",
    "        return logits * self.s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_arcface_loss(input_shape=(150, 150, 3), n_classes=2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # No activation on the last dense layer\n",
    "    x = Dense(128)(x)  # This is the embedding layer\n",
    "    # Note: ArcFace layer is typically applied during training only\n",
    "\n",
    "    # Placeholder for labels\n",
    "    labels = Input(shape=(n_classes,))\n",
    "    # Instantiate ArcFace loss layer\n",
    "    arcface_loss = ArcFaceLossLayer(n_classes=n_classes)(x, labels)\n",
    "\n",
    "    model = Model(inputs=[inputs, labels], outputs=arcface_loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_with_arcface_loss(input_shape=(150, 150, 3), n_classes=2)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss=[tf.nn.softmax_cross_entropy_with_logits],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# When fitting the model, ensure to pass both the images and labels as input,\n",
    "# and similarly for validation. You might need to adjust your data generator\n",
    "# or data loading mechanism to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 11s 254ms/step - loss: 6.4804 - accuracy: 0.5211 - val_loss: 4.6905 - val_accuracy: 0.7396\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 11s 263ms/step - loss: 5.1538 - accuracy: 0.5187 - val_loss: 3.9199 - val_accuracy: 0.7201\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 11s 266ms/step - loss: 3.7971 - accuracy: 0.5823 - val_loss: 3.4966 - val_accuracy: 0.7388\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 12s 288ms/step - loss: 2.6691 - accuracy: 0.6811 - val_loss: 2.2151 - val_accuracy: 0.7799\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 12s 283ms/step - loss: 1.9128 - accuracy: 0.7601 - val_loss: 1.4451 - val_accuracy: 0.8134\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 12s 279ms/step - loss: 1.3222 - accuracy: 0.8098 - val_loss: 1.5842 - val_accuracy: 0.8321\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 13s 297ms/step - loss: 1.1273 - accuracy: 0.8332 - val_loss: 1.1891 - val_accuracy: 0.8918\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 13s 307ms/step - loss: 0.8025 - accuracy: 0.8844 - val_loss: 1.0709 - val_accuracy: 0.8731\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 14s 323ms/step - loss: 0.6676 - accuracy: 0.8932 - val_loss: 1.2806 - val_accuracy: 0.8731\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 13s 312ms/step - loss: 0.4986 - accuracy: 0.9115 - val_loss: 1.0669 - val_accuracy: 0.8993\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 13s 295ms/step - loss: 0.2854 - accuracy: 0.9451 - val_loss: 1.1845 - val_accuracy: 0.8819\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 13s 292ms/step - loss: 0.1865 - accuracy: 0.9620 - val_loss: 0.9343 - val_accuracy: 0.9179\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 13s 299ms/step - loss: 0.0885 - accuracy: 0.9788 - val_loss: 1.2921 - val_accuracy: 0.9142\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 13s 302ms/step - loss: 0.1146 - accuracy: 0.9744 - val_loss: 1.1437 - val_accuracy: 0.9030\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 13s 300ms/step - loss: 0.0775 - accuracy: 0.9810 - val_loss: 1.5148 - val_accuracy: 0.8843\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 13s 302ms/step - loss: 0.0417 - accuracy: 0.9898 - val_loss: 1.4042 - val_accuracy: 0.9216\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 13s 304ms/step - loss: 0.0344 - accuracy: 0.9927 - val_loss: 0.8028 - val_accuracy: 0.9328\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 13s 304ms/step - loss: 0.0218 - accuracy: 0.9927 - val_loss: 1.5136 - val_accuracy: 0.8955\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 13s 304ms/step - loss: 0.0121 - accuracy: 0.9978 - val_loss: 1.0929 - val_accuracy: 0.9104\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 13s 305ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 1.5975 - val_accuracy: 0.9067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x374405710>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_gen_wrapper,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_gen_wrapper,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 70ms/step - loss: 1.6139 - accuracy: 0.9028\n",
      "Test Loss: 1.6139473915100098, Test Accuracy: 0.9027777910232544\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_gen_wrapper, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka",
   "language": "python",
   "name": "magisterka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
